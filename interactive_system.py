"""
interactive_system.py
å¿«é€Ÿå¯åŠ¨çš„RAGäº¤äº’ç³»ç»Ÿ - é€‚é…1024ç»´åº¦é«˜è´¨é‡ä¸­æ–‡æ¨¡å‹
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

import json
import time
from typing import List, Dict, Optional, Any, Tuple

import pandas as pd
import yaml

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from src.rag.engine import RAGEngine
from src.rag.embedding import EmbeddingModel

# å¯¼å…¥åˆ†ææ¨¡å— - ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
from src.analysis.author_analyzer import AuthorAnalyzer
from src.analysis.concept_analyzer import ConceptAnalyzer


class FastRAGInteractiveSystem:
    """å¿«é€Ÿå¯åŠ¨çš„RAGäº¤äº’ç³»ç»Ÿ - 1024ç»´æ¨¡å‹ä¼˜åŒ–ç‰ˆ"""

    def __init__(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        self.print_header()
        self.load_config()
        self.init_embedding_model()
        self.init_rag_engine()
        self.check_database_status()

        # å»¶è¿Ÿåˆå§‹åŒ–åˆ†æå™¨
        self.author_analyzer = None
        self.concept_analyzer = None

        print("\nâœ… ç³»ç»Ÿå¯åŠ¨å®Œæˆï¼")

    def print_header(self):
        """æ‰“å°ç³»ç»Ÿæ ‡é¢˜"""
        print("=" * 70)
        print("ğŸ¨ è‰ºæœ¯è®¾è®¡RAGç³»ç»Ÿ v2.0 - é«˜ç²¾åº¦ç‰ˆ")
        print("=" * 70)
        print("æ ¸å¿ƒåŠŸèƒ½ï¼š")
        print("  âœ“ 1024ç»´é«˜è´¨é‡ä¸­æ–‡åµŒå…¥æ¨¡å‹")
        print("  âœ“ æ™ºèƒ½æ–‡æ¡£æ£€ç´¢ (æ··åˆç®—æ³•)")
        print("  âœ“ ä½œè€…æœç´¢ä¸åˆ†æï¼ˆå»¶è¿ŸåŠ è½½ï¼‰")
        print("  âœ“ æ¦‚å¿µåˆ†æä¸æ¼”è¿›ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰")
        print("  âœ“ ä¼˜åŒ–çš„å†…å­˜ç®¡ç†")
        print("=" * 70)

    def load_config(self):
        """åŠ è½½é…ç½®"""
        try:
            # å°è¯•å¤šä¸ªé…ç½®æ–‡ä»¶
            config_files = ['config.yaml', 'config.yaml.backup']
            self.config = {}

            for config_path in config_files:
                if Path(config_path).exists():
                    with open(config_path, 'r', encoding='utf-8') as f:
                        self.config = yaml.safe_load(f)
                    print(f"âœ“ åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
                    break

            if not self.config:
                print("âš ï¸  æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                self.config = self.get_default_config()

        except Exception as e:
            print(f"âš ï¸  åŠ è½½é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            self.config = self.get_default_config()

    def get_default_config(self):
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'embedding': {
                'current_model': 'bge_large',
                'models': {
                    'bge_large': {
                        'model_name': 'BAAI/bge-large-zh-v1.5',
                        'dimension': 1024,
                        'device': 'cuda'
                    }
                },
                'mixed_precision': True,
                'batch_size': 32
            },
            'vector_store': {
                'collection_name': 'art_design_docs_v2',
                'persist_directory': 'data/chroma_v2'
            },
            'retrieval': {
                'default_top_k': 5,
                'bm25_weight': 0.3,
                'vector_weight': 0.7
            }
        }

    def init_embedding_model(self):
        """åˆå§‹åŒ–é«˜è´¨é‡åµŒå…¥æ¨¡å‹"""
        print("\nğŸ¤– åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")

        # ä»é…ç½®è·å–æ¨¡å‹ä¿¡æ¯
        embedding_config = self.config.get('embedding', {})
        current_model = embedding_config.get('current_model', 'bge_large')
        models_config = embedding_config.get('models', {})

        if current_model not in models_config:
            print("âš ï¸  é…ç½®çš„æ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤BGE-Largeæ¨¡å‹")
            current_model = 'bge_large'
            models_config = self.get_default_config()['embedding']['models']

        model_config = models_config[current_model]

        try:
            self.embedding_model = EmbeddingModel(
                model_name=model_config['model_name'],
                device=model_config.get('device', 'cuda'),
                use_fp16=embedding_config.get('mixed_precision', True),
                batch_size=embedding_config.get('batch_size', 32)
            )

            print(f"âœ“ åµŒå…¥æ¨¡å‹: {model_config['model_name']}")
            print(f"âœ“ æ¨¡å‹ç»´åº¦: {self.embedding_model.embedding_dim}")
            print(f"âœ“ è®¾å¤‡: {self.embedding_model.device}")
            print(f"âœ“ æ‰¹æ¬¡å¤§å°: {self.embedding_model.batch_size}")

            # éªŒè¯ç»´åº¦
            if self.embedding_model.embedding_dim != model_config['dimension']:
                print(f"âš ï¸  å®é™…ç»´åº¦({self.embedding_model.embedding_dim}) != é…ç½®ç»´åº¦({model_config['dimension']})")

        except Exception as e:
            print(f"âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            print("å›é€€åˆ°é»˜è®¤æ¨¡å‹...")
            self.embedding_model = EmbeddingModel(model_type='bge-large-zh')

    def init_rag_engine(self):
        """åˆå§‹åŒ–RAGå¼•æ“"""
        print("\nğŸ”§ åˆå§‹åŒ–RAGå¼•æ“...")

        api_key = os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY')

        # è·å–å‘é‡å­˜å‚¨é…ç½®
        vector_config = self.config.get('vector_store', {})
        collection_name = vector_config.get('collection_name', 'art_design_docs_v2')
        persist_directory = vector_config.get('persist_directory', 'data/chroma_v2')

        # ä½¿ç”¨é¢„åˆå§‹åŒ–çš„åµŒå…¥æ¨¡å‹åˆ›å»ºRAGå¼•æ“
        self.rag = RAGEngine(
            collection_name=collection_name,
            persist_directory=persist_directory,
            embedding_model=None,  # ç¨åæ‰‹åŠ¨è®¾ç½®
            gemini_api_key=api_key,
            enable_bm25=True
        )

        # æ‰‹åŠ¨è®¾ç½®åµŒå…¥æ¨¡å‹
        self.rag.embedding_model = self.embedding_model

        # é‡æ–°åˆå§‹åŒ–æ£€ç´¢å™¨ä»¥ä½¿ç”¨æ–°çš„åµŒå…¥æ¨¡å‹
        self.rag.retriever.embedding_model = self.embedding_model

        print(f"âœ“ RAGå¼•æ“åˆå§‹åŒ–å®Œæˆ")
        print(f"âœ“ é›†åˆåç§°: {collection_name}")
        print(f"âœ“ å­˜å‚¨ç›®å½•: {persist_directory}")

        if api_key:
            print("âœ“ Gemini APIå·²é…ç½®")
        else:
            print("âš ï¸  æœªé…ç½®Gemini APIï¼ˆå¯é€‰ï¼‰")

    def check_database_status(self):
        """æ£€æŸ¥æ•°æ®åº“çŠ¶æ€"""
        print("\nğŸ—„ï¸  æ£€æŸ¥æ•°æ®åº“çŠ¶æ€...")

        doc_count = self.rag.vector_store.count()

        if doc_count == 0:
            print("\nâŒ å‘é‡æ•°æ®åº“ä¸ºç©ºï¼")
            print("è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤æ„å»ºç´¢å¼•ï¼š")
            print("  python build_rag.py --input data/raw/applied_arts.xlsx")
            print("  æˆ–")
            print("  python test_preprocessing.py")

            # è¯¢é—®æ˜¯å¦ç»§ç»­
            choice = input("\næ˜¯å¦ç»§ç»­å¯åŠ¨ç³»ç»Ÿï¼Ÿ(y/n): ").strip().lower()
            if choice != 'y':
                sys.exit(1)
            else:
                print("âš ï¸  ç³»ç»Ÿå°†ä»¥æœ‰é™åŠŸèƒ½å¯åŠ¨")
        else:
            print(f"âœ“ å‘é‡æ•°æ®åº“å°±ç»ªï¼ˆ{doc_count:,}ä¸ªæ–‡æ¡£ï¼‰")

        # æ£€æŸ¥BM25ç´¢å¼•
        if hasattr(self.rag, 'retriever') and self.rag.retriever.bm25_index:
            bm25_docs = len(self.rag.retriever.documents)
            print(f"âœ“ BM25ç´¢å¼•å°±ç»ªï¼ˆ{bm25_docs:,}ä¸ªæ–‡æ¡£ï¼‰")
        else:
            print("âš ï¸  BM25ç´¢å¼•æœªæ„å»ºï¼ˆå°†å½±å“æ··åˆæ£€ç´¢æ•ˆæœï¼‰")

            if doc_count > 0:
                print("ğŸ’¡ æç¤º: é¦–æ¬¡ä½¿ç”¨æ··åˆæ£€ç´¢æ—¶å°†è‡ªåŠ¨æ„å»ºBM25ç´¢å¼•")

    def init_author_analyzer(self):
        """å»¶è¿Ÿåˆå§‹åŒ–ä½œè€…åˆ†æå™¨"""
        if self.author_analyzer is None:
            print("\nğŸ”§ é¦–æ¬¡ä½¿ç”¨ä½œè€…åŠŸèƒ½ï¼Œåˆå§‹åŒ–åˆ†æå™¨...")
            start_time = time.time()

            # ä½¿ç”¨å»¶è¿ŸåŠ è½½çš„ä¼˜åŒ–ç‰ˆæœ¬
            self.author_analyzer = AuthorAnalyzer(self.rag, lazy_load=True)

            elapsed = time.time() - start_time
            print(f"âœ“ ä½œè€…åˆ†æå™¨å°±ç»ª ({elapsed:.1f}ç§’)")

    def init_concept_analyzer(self):
        """å»¶è¿Ÿåˆå§‹åŒ–æ¦‚å¿µåˆ†æå™¨"""
        if self.concept_analyzer is None:
            print("\nğŸ”§ é¦–æ¬¡ä½¿ç”¨æ¦‚å¿µåŠŸèƒ½ï¼Œåˆå§‹åŒ–åˆ†æå™¨...")
            start_time = time.time()

            self.concept_analyzer = ConceptAnalyzer(self.rag)

            elapsed = time.time() - start_time
            print(f"âœ“ æ¦‚å¿µåˆ†æå™¨å°±ç»ª ({elapsed:.1f}ç§’)")

    def run(self):
        """è¿è¡Œä¸»å¾ªç¯"""
        print("\n" + "=" * 70)
        print("ğŸš€ ç³»ç»Ÿå°±ç»ªï¼è¾“å…¥ 'help' æŸ¥çœ‹å‘½ä»¤åˆ—è¡¨")
        print("ğŸ’¡ æç¤º: é¦–æ¬¡æœç´¢å¯èƒ½éœ€è¦å‡ ç§’é’Ÿæ„å»ºç´¢å¼•")
        print("=" * 70)

        while True:
            try:
                query = input("\nğŸ” è¾“å…¥> ").strip()

                if not query:
                    continue

                # å¤„ç†é€€å‡ºå‘½ä»¤
                if query.lower() in ['exit', 'quit', 'q']:
                    print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                    break

                # å¤„ç†å¸®åŠ©å‘½ä»¤
                if query.lower() in ['help', 'h', '?']:
                    self.show_help()
                    continue

                # å¤„ç†ç³»ç»Ÿå‘½ä»¤
                if query.lower() == 'stats':
                    self.show_system_stats()
                    continue

                if query.lower() == 'authors':
                    self.show_author_list()
                    continue

                if query.lower() == 'test':
                    self.run_tests()
                    continue

                if query.lower() == 'clear':
                    os.system('cls' if os.name == 'nt' else 'clear')
                    self.print_header()
                    continue

                # å¤„ç†åˆ†æå‘½ä»¤
                if query.startswith('/'):
                    self.handle_command(query)
                    continue

                # é»˜è®¤ï¼šæ™®é€šæœç´¢
                self.handle_search(query)

            except KeyboardInterrupt:
                print("\n\nä½¿ç”¨ 'exit' é€€å‡ºç¨‹åº")
            except Exception as e:
                print(f"\nâŒ é”™è¯¯: {e}")
                # åœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†é”™è¯¯
                if os.getenv('DEBUG'):
                    import traceback
                    traceback.print_exc()

    def show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        print("\n" + "=" * 70)
        print("ğŸ“š å‘½ä»¤å¸®åŠ©")
        print("=" * 70)

        print("\nğŸ” åŸºç¡€æœç´¢:")
        print("  ç›´æ¥è¾“å…¥å…³é”®è¯è¿›è¡Œæ–‡æ¡£æœç´¢")
        print("  ä¾‹å¦‚: åŒ…è±ªæ–¯è®¾è®¡ç†å¿µ")

        print("\nğŸ‘¤ ä½œè€…åˆ†æå‘½ä»¤:")
        print("  /author <ä½œè€…å>        - åˆ†ææŒ‡å®šä½œè€…çš„æ‰€æœ‰æ–‡ç« ")
        print("  /find_author <å…³é”®è¯>   - æœç´¢åŒ…å«å…³é”®è¯çš„ä½œè€…")

        print("\nğŸ’¡ æ¦‚å¿µåˆ†æå‘½ä»¤:")
        print("  /concept <æ¦‚å¿µ>         - åˆ†ææ¦‚å¿µçš„è¯¦ç»†ä¿¡æ¯")
        print("  /first <æ¦‚å¿µ>           - æŸ¥æ‰¾æ¦‚å¿µçš„é¦–æ¬¡å‡ºç°")
        print("  /related <æ¦‚å¿µ>         - æŸ¥æ‰¾ç›¸å…³æ¦‚å¿µ")
        print("  /evolution <æ¦‚å¿µ>       - åˆ†ææ¦‚å¿µæ¼”è¿›")

        print("\nâš™ï¸  ç³»ç»Ÿå‘½ä»¤:")
        print("  stats                   - æ˜¾ç¤ºç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯")
        print("  authors                 - æ˜¾ç¤ºä½œè€…åˆ—è¡¨")
        print("  test                    - è¿è¡ŒåŠŸèƒ½æµ‹è¯•")
        print("  clear                   - æ¸…å±")
        print("  help                    - æ˜¾ç¤ºæ­¤å¸®åŠ©")
        print("  exit                    - é€€å‡ºç¨‹åº")

        print("\nğŸ’¡ æœç´¢æŠ€å·§:")
        print("  â€¢ ä½¿ç”¨å…·ä½“çš„æ¦‚å¿µè¯æ±‡å¯ä»¥è·å¾—æ›´ç²¾ç¡®çš„ç»“æœ")
        print("  â€¢ 1024ç»´æ¨¡å‹å¯¹ä¸­æ–‡è¯­ä¹‰ç†è§£æ›´å‡†ç¡®")
        print("  â€¢ æ··åˆæ£€ç´¢ç»“åˆäº†è¯­ä¹‰å’Œå…³é”®è¯åŒ¹é…")

    def handle_command(self, command: str):
        """å¤„ç†å‘½ä»¤"""
        parts = command.split(maxsplit=1)
        cmd = parts[0].lower()
        args = parts[1] if len(parts) > 1 else ""

        print(f"\nğŸ¯ æ‰§è¡Œå‘½ä»¤: {cmd}")

        # ä½œè€…ç›¸å…³å‘½ä»¤
        if cmd == '/author':
            if args:
                self.init_author_analyzer()  # å»¶è¿Ÿåˆå§‹åŒ–
                self.analyze_author(args)
            else:
                print("âŒ è¯·æŒ‡å®šä½œè€…å: /author ä½œè€…å")

        elif cmd == '/find_author':
            if args:
                self.init_author_analyzer()  # å»¶è¿Ÿåˆå§‹åŒ–
                self.find_authors(args)
            else:
                print("âŒ è¯·æŒ‡å®šæœç´¢å…³é”®è¯: /find_author å…³é”®è¯")

        # æ¦‚å¿µç›¸å…³å‘½ä»¤
        elif cmd == '/concept':
            if args:
                self.init_concept_analyzer()  # å»¶è¿Ÿåˆå§‹åŒ–
                self.analyze_concept(args)
            else:
                print("âŒ è¯·æŒ‡å®šæ¦‚å¿µ: /concept æ¦‚å¿µå")

        elif cmd == '/first':
            if args:
                self.init_concept_analyzer()  # å»¶è¿Ÿåˆå§‹åŒ–
                self.find_first_appearance(args)
            else:
                print("âŒ è¯·æŒ‡å®šæ¦‚å¿µ: /first æ¦‚å¿µå")

        elif cmd == '/related':
            if args:
                self.init_concept_analyzer()  # å»¶è¿Ÿåˆå§‹åŒ–
                self.find_related_concepts(args)
            else:
                print("âŒ è¯·æŒ‡å®šæ¦‚å¿µ: /related æ¦‚å¿µå")

        elif cmd == '/evolution':
            if args:
                self.init_concept_analyzer()  # å»¶è¿Ÿåˆå§‹åŒ–
                self.analyze_concept_evolution(args)
            else:
                print("âŒ è¯·æŒ‡å®šæ¦‚å¿µ: /evolution æ¦‚å¿µå")

        else:
            print(f"âŒ æœªçŸ¥å‘½ä»¤: {cmd}")
            print("è¾“å…¥ 'help' æŸ¥çœ‹å¯ç”¨å‘½ä»¤")

    def handle_search(self, query: str):
        """å¤„ç†æ™®é€šæœç´¢ - ä¼˜åŒ–ç‰ˆ"""
        print(f"ğŸ” æœç´¢: {query}")

        # è·å–é…ç½®çš„æ£€ç´¢å‚æ•°
        retrieval_config = self.config.get('retrieval', {})
        top_k = retrieval_config.get('default_top_k', 5)
        bm25_weight = retrieval_config.get('bm25_weight', 0.3)
        vector_weight = retrieval_config.get('vector_weight', 0.7)

        start_time = time.time()

        try:
            results, stats = self.rag.hybrid_search(
                query=query,
                top_k=top_k,
                method="hybrid",
                bm25_weight=bm25_weight,
                vector_weight=vector_weight,
                rerank=True
            )

            elapsed = time.time() - start_time

            print(f"â±ï¸  æœç´¢å®Œæˆ ({elapsed:.2f}ç§’)")
            print(f"   æ–¹æ³•: {stats.get('method', 'hybrid')}")
            print(f"   ç»“æœæ•°: {len(results)}")

            # æ˜¾ç¤ºæ£€ç´¢ç»Ÿè®¡
            if 'vector_results' in stats:
                print(f"   å‘é‡æ£€ç´¢: {stats['vector_results']}ä¸ª")
            if 'bm25_results' in stats:
                print(f"   BM25æ£€ç´¢: {stats['bm25_results']}ä¸ª")

            if not results:
                print("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                print("ğŸ’¡ å°è¯•ä½¿ç”¨æ›´é€šç”¨çš„å…³é”®è¯æˆ–æ£€æŸ¥æ‹¼å†™")
                return

            print("\n" + "=" * 60)
            for i, doc in enumerate(results, 1):
                self._display_search_result(i, doc)

            # è¯¢é—®æ˜¯å¦ç”ŸæˆAIç­”æ¡ˆ
            if self.rag.generator.is_available() and len(results) > 0:
                print("\n" + "-" * 60)
                choice = input("ğŸ’¡ æ˜¯å¦éœ€è¦AIç”Ÿæˆç»¼åˆç­”æ¡ˆï¼Ÿ(y/n): ").strip().lower()
                if choice == 'y':
                    self._generate_ai_answer(query, results[:3])

        except Exception as e:
            elapsed = time.time() - start_time
            print(f"âŒ æœç´¢å¤±è´¥ ({elapsed:.2f}ç§’): {e}")

            # æä¾›æ•…éšœæ’é™¤å»ºè®®
            if "ChromaDB" in str(e) or "collection" in str(e):
                print("ğŸ’¡ å¯èƒ½æ˜¯æ•°æ®åº“é—®é¢˜ï¼Œè¯·æ£€æŸ¥å‘é‡åº“æ˜¯å¦æ­£ç¡®æ„å»º")
            elif "CUDA" in str(e) or "memory" in str(e):
                print("ğŸ’¡ å¯èƒ½æ˜¯GPUå†…å­˜ä¸è¶³ï¼Œå¯å°è¯•å‡å°æ‰¹æ¬¡å¤§å°")

    def _display_search_result(self, rank: int, doc: Dict):
        """æ˜¾ç¤ºæœç´¢ç»“æœ - å¢å¼ºç‰ˆ"""
        metadata = doc.get('metadata', {})

        # è·å–å„ç§åˆ†æ•°
        scores = []
        if 'rerank_score' in doc:
            scores.append(f"é‡æ’åº: {doc['rerank_score']:.3f}")
        if 'combined_score' in doc:
            scores.append(f"ç»¼åˆ: {doc['combined_score']:.3f}")
        if 'score' in doc:
            scores.append(f"åŸå§‹: {doc['score']:.3f}")

        score_str = " | ".join(scores) if scores else "N/A"

        print(f"\nã€{rank}ã€‘{metadata.get('æ–‡ç« åç§°+å‰¯æ ‡é¢˜', 'æ— æ ‡é¢˜')}")
        print(f"   ğŸ“… å¹´ä»½: {metadata.get('å¹´ä»½', 'N/A')}")
        print(f"   ğŸ‘¤ ä½œè€…: {metadata.get('ä½œè€…åç§°', 'N/A')}")
        print(f"   ğŸ“‚ åˆ†ç±»: {metadata.get('åˆ†ç±»', 'N/A')}")
        print(f"   ğŸ“Š ç›¸å…³åº¦: {score_str}")

        # æ˜¾ç¤ºæ£€ç´¢æ–¹æ³•ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'retrieval_method' in doc:
            print(f"   ğŸ” æ£€ç´¢æ–¹å¼: {doc['retrieval_method']}")

        # æ˜¾ç¤ºæ–‡æœ¬ç‰‡æ®µ
        text = doc.get('text', '')[:300]
        print(f"   ğŸ“ æ‘˜è¦: {text}...")

        # æ˜¾ç¤ºchunkä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'chunk_strategy' in metadata:
            print(f"   ğŸ§© å—ç­–ç•¥: {metadata['chunk_strategy']}")

    # def _generate_ai_answer(self, query: str, context: List[Dict]):
    #     """ç”ŸæˆAIç­”æ¡ˆ"""
    #     print("\nğŸ¤– æ­£åœ¨ç”ŸæˆAIç­”æ¡ˆ...")
    #
    #     try:
    #         answer = self.rag.generate_answer_with_citations(query, context)
    #         print("\n" + "=" * 60)
    #         print("ğŸ’¡ AIç»¼åˆç­”æ¡ˆ:")
    #         print("=" * 60)
    #         print(answer)
    #         print("=" * 60)
    #     except Exception as e:
    #         print(f"âŒ ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
    #         if "API" in str(e):
    #             print("ğŸ’¡ è¯·æ£€æŸ¥APIå¯†é’¥é…ç½®")

    def _generate_ai_answer(self, query: str, context: List[Dict]):
        """ç”ŸæˆAIç­”æ¡ˆ - ä¿®å¤ç‰ˆæœ¬"""
        print("\nğŸ¤– æ­£åœ¨ç”ŸæˆAIç­”æ¡ˆ...")

        try:
            # æ–¹æ³•1ï¼šä½¿ç”¨ generate_with_citations
            answer = self.rag.generate_answer_with_citations(query, context)

            print("\n" + "=" * 60)
            print("ğŸ’¡ AIç»¼åˆç­”æ¡ˆ:")
            print("=" * 60)
            print(answer)
            print("=" * 60)

        except Exception as e:
            # è¯¦ç»†çš„é”™è¯¯å¤„ç†
            error_msg = str(e)
            print(f"âŒ ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {error_msg}")

            # é’ˆå¯¹ç‰¹å®šé”™è¯¯æä¾›è§£å†³æ–¹æ¡ˆ
            if "Invalid format specifier" in error_msg:
                print("ğŸ’¡ æ£€æµ‹åˆ°æ ¼å¼åŒ–é”™è¯¯ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...")

                # å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥è°ƒç”¨åŸºç¡€ generate æ–¹æ³•
                try:
                    # ä½¿ç”¨æ›´ç®€å•çš„æç¤ºè¯ï¼Œé¿å…å¤æ‚æ ¼å¼
                    simple_prompt = """ä½ æ˜¯è‰ºæœ¯è®¾è®¡é¢†åŸŸçš„ä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å›ç­”é—®é¢˜ã€‚

    é‡è¦è§„åˆ™ï¼š
    1. åªä½¿ç”¨æ–‡æ¡£ä¸­çš„ä¿¡æ¯
    2. ä¿æŒä¸“ä¸šå’Œå‡†ç¡®
    3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜"""

                    answer = self.rag.generator.generate(
                        query=query,
                        context=context,
                        system_prompt=simple_prompt
                    )

                    print("\n" + "=" * 60)
                    print("ğŸ’¡ AIç»¼åˆç­”æ¡ˆï¼ˆç®€åŒ–ç‰ˆï¼‰:")
                    print("=" * 60)
                    print(answer)
                    print("=" * 60)

                except Exception as e2:
                    print(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {e2}")
                    self._show_manual_summary(query, context)

            elif "API" in error_msg:
                print("ğŸ’¡ è¯·æ£€æŸ¥APIå¯†é’¥é…ç½®")
                print("   è®¾ç½®ç¯å¢ƒå˜é‡: export GOOGLE_API_KEY='your_key'")
            else:
                # æ˜¾ç¤ºæ›´å¤šè°ƒè¯•ä¿¡æ¯
                if os.getenv('DEBUG'):
                    import traceback
                    traceback.print_exc()

                # æä¾›æ‰‹åŠ¨æ‘˜è¦
                self._show_manual_summary(query, context)

    def _show_manual_summary(self, query: str, context: List[Dict]):
        """æ˜¾ç¤ºæ‰‹åŠ¨æ‘˜è¦ä½œä¸ºåå¤‡æ–¹æ¡ˆ"""
        print("\n" + "=" * 60)
        print("ğŸ“‹ ç›¸å…³æ–‡æ¡£æ‘˜è¦ï¼ˆæ‰‹åŠ¨æ•´ç†ï¼‰:")
        print("=" * 60)

        print(f"å…³äºã€Œ{query}ã€çš„ç›¸å…³ä¿¡æ¯ï¼š\n")

        for i, doc in enumerate(context[:3], 1):
            metadata = doc.get('metadata', {})
            text = doc.get('text', '')

            print(f"{i}. ã€Š{metadata.get('æ–‡ç« åç§°+å‰¯æ ‡é¢˜', 'æ— æ ‡é¢˜')}ã€‹")
            print(f"   ä½œè€…ï¼š{metadata.get('ä½œè€…åç§°', 'æœªçŸ¥')}")
            print(f"   å¹´ä»½ï¼š{metadata.get('å¹´ä»½', 'æœªçŸ¥')}")
            print(f"   ç›¸å…³å†…å®¹ï¼š")

            # æå–å…³é”®å¥å­
            sentences = text.split('ã€‚')
            relevant_sentences = []

            # ç®€å•çš„ç›¸å…³æ€§åˆ¤æ–­
            query_words = set(query.split())
            for sent in sentences[:10]:  # åªçœ‹å‰10ä¸ªå¥å­
                if any(word in sent for word in query_words):
                    relevant_sentences.append(sent.strip())

            if relevant_sentences:
                for sent in relevant_sentences[:3]:
                    print(f"   â€¢ {sent}ã€‚")
            else:
                print(f"   â€¢ {text[:200]}...")

            print()

        print("=" * 60)
        print("ğŸ’¡ æç¤ºï¼šè¿™æ˜¯åŸºäºæ–‡æ¡£çš„ç®€å•æ‘˜è¦ã€‚å¯ç”¨AIåŠŸèƒ½å¯è·å¾—æ›´å¥½çš„ç»¼åˆç­”æ¡ˆã€‚")

    def analyze_author(self, author_name: str):
        """åˆ†æä½œè€…"""
        print(f"ğŸ“– åˆ†æä½œè€…: {author_name}")

        start_time = time.time()
        result = self.author_analyzer.analyze_author(author_name)
        elapsed = time.time() - start_time

        print(f"â±ï¸  åˆ†æå®Œæˆ ({elapsed:.2f}ç§’)")

        if result['status'] == 'not_found':
            print(f"âŒ {result['message']}")

            # æä¾›ç›¸ä¼¼ä½œè€…å»ºè®®
            similar = self.author_analyzer.search_authors(author_name)
            if similar:
                print(f"\nğŸ’¡ æ˜¯å¦è¦æŸ¥æ‰¾ä»¥ä¸‹ç›¸ä¼¼ä½œè€…ï¼Ÿ")
                for i, (author, count) in enumerate(similar[:5], 1):
                    print(f"   {i}. {author} ({count}ç¯‡)")
            return

        # æ˜¾ç¤ºåˆ†æç»“æœ
        print(f"\nâœ… æ‰¾åˆ°ä½œè€…: {result['matched_author']}")
        if result['query_author'] != result['matched_author']:
            print(f"   (æŸ¥è¯¢: {result['query_author']})")

        print(f"   ğŸ¯ åŒ¹é…åº¦: {result['match_confidence']:.2%}")
        print(f"   ğŸ“š å‘æ–‡æ€»æ•°: {result['total_publications']}")

        # æ˜¾ç¤ºå‘è¡¨æ–‡ç« 
        print(f"\nğŸ“„ å‘è¡¨æ–‡ç« :")
        for i, pub in enumerate(result['publications'][:15], 1):
            year_str = f"[{pub['year']}]" if pub['year'] else "[æœªçŸ¥]"
            category_str = f" - {pub['category']}" if pub['category'] else ""
            print(f"   {i:2d}. {year_str} {pub['title']}{category_str}")

        if len(result['publications']) > 15:
            print(f"   ... è¿˜æœ‰ {len(result['publications']) - 15} ç¯‡æ–‡ç« ")

        # æ˜¾ç¤ºå¹´ä»½åˆ†å¸ƒ
        if result['year_distribution']:
            print(f"\nğŸ“Š å¹´ä»½åˆ†å¸ƒ:")
            sorted_years = sorted(result['year_distribution'].items())
            for year, count in sorted_years:
                bar = 'â–ˆ' * min(count, 20)
                print(f"   {year}: {bar} ({count}ç¯‡)")

        # æ˜¾ç¤ºåˆ†ç±»åˆ†å¸ƒ
        if result['category_distribution']:
            print(f"\nğŸ“š åˆ†ç±»åˆ†å¸ƒ:")
            for category, count in list(result['category_distribution'].items())[:5]:
                print(f"   {category}: {count}ç¯‡")

        # æ˜¾ç¤ºåˆä½œè€…
        if result['collaborators']:
            print(f"\nğŸ‘¥ ä¸»è¦åˆä½œè€…:")
            for collaborator, count in list(result['collaborators'].items())[:8]:
                print(f"   â€¢ {collaborator} ({count}æ¬¡)")

        # æ˜¾ç¤ºç ”ç©¶ä¸»é¢˜
        if result['research_topics']:
            print(f"\nğŸ¯ ç ”ç©¶ä¸»é¢˜:")
            for topic, count in list(result['research_topics'].items())[:8]:
                print(f"   â€¢ {topic} ({count}æ¬¡)")

    def find_authors(self, pattern: str):
        """æœç´¢ä½œè€…"""
        print(f"ğŸ” æœç´¢ä½œè€…: {pattern}")

        matches = self.author_analyzer.search_authors(pattern)

        if not matches:
            print(f"âŒ æœªæ‰¾åˆ°åŒ¹é… '{pattern}' çš„ä½œè€…")
            return

        print(f"\nâœ… æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…çš„ä½œè€…:")
        for i, (author, count) in enumerate(matches[:20], 1):
            print(f"   {i:2d}. {author:<15} ({count}ç¯‡)")

        if len(matches) > 20:
            print(f"   ... è¿˜æœ‰ {len(matches) - 20} ä¸ªä½œè€…")

    def analyze_concept(self, concept: str):
        """åˆ†ææ¦‚å¿µ"""
        print(f"ğŸ’¡ åˆ†ææ¦‚å¿µ: {concept}")

        start_time = time.time()
        report = self.concept_analyzer.generate_concept_report(concept)
        elapsed = time.time() - start_time

        print(f"â±ï¸  åˆ†æå®Œæˆ ({elapsed:.2f}ç§’)")

        if report['status'] == 'not_found':
            print(f"âŒ æœªæ‰¾åˆ°æ¦‚å¿µ '{concept}' çš„ç›¸å…³æ–‡æ¡£")
            return

        # æ˜¾ç¤ºé¦–æ¬¡å‡ºç°
        first = report.get('first_appearance', {})
        if first.get('status') == 'found':
            print(f"\nğŸ“ é¦–æ¬¡å‡ºç°:")
            print(f"   å¹´ä»½: {first['year']}")
            print(f"   æ–‡çŒ®: ã€Š{first['title']}ã€‹")
            print(f"   ä½œè€…: {first['author']}")
            print(f"   æ‰¾åˆ°æ–‡æ¡£: {first['total_docs_found']}ç¯‡")

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = report.get('statistics', {})
        if stats:
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   æ€»æåŠæ¬¡æ•°: {stats.get('total_mentions', 0)}")
            print(f"   æ´»è·ƒå¹´ä»½æ•°: {stats.get('active_years', 0)}")
            print(f"   å¹´ä»½èŒƒå›´: {stats.get('year_range', 'N/A')}")
            print(f"   å³°å€¼å¹´ä»½: {stats.get('peak_year', 'N/A')} ({stats.get('peak_count', 0)}ç¯‡)")

        # æ˜¾ç¤ºç›¸å…³æ¦‚å¿µ
        related = report.get('related_concepts', [])
        if related:
            print(f"\nğŸ”— ç›¸å…³æ¦‚å¿µ:")
            for concept_name, count in related[:8]:
                print(f"   â€¢ {concept_name} (å…±ç°{count}æ¬¡)")

    def find_first_appearance(self, concept: str):
        """æŸ¥æ‰¾æ¦‚å¿µé¦–æ¬¡å‡ºç°"""
        print(f"ğŸ“ æŸ¥æ‰¾æ¦‚å¿µé¦–æ¬¡å‡ºç°: {concept}")

        result = self.concept_analyzer.find_first_appearance(concept)

        if result['status'] == 'found':
            print(f"\nâœ… æ‰¾åˆ°é¦–æ¬¡å‡ºç°:")
            print(f"   å¹´ä»½: {result['year']}")
            print(f"   æ–‡çŒ®: ã€Š{result['title']}ã€‹")
            print(f"   ä½œè€…: {result['author']}")
            print(f"   åˆ†ç±»: {result.get('category', 'æœªçŸ¥')}")
            print(f"   æ‰¾åˆ°æ–‡æ¡£: {result['total_docs_found']}ç¯‡")
            print(f"\nğŸ“ ä¸Šä¸‹æ–‡:")
            print(f"   {result['context']}")

        elif result['status'] == 'no_valid_date':
            print(f"âš ï¸  {result['message']}")

        else:
            print(f"âŒ {result['message']}")

    def find_related_concepts(self, concept: str):
        """æŸ¥æ‰¾ç›¸å…³æ¦‚å¿µ"""
        print(f"ğŸ”— æŸ¥æ‰¾ç›¸å…³æ¦‚å¿µ: {concept}")

        related = self.concept_analyzer.find_related_concepts(concept, top_n=15)

        if not related:
            print(f"âŒ æœªæ‰¾åˆ°ä¸ '{concept}' ç›¸å…³çš„æ¦‚å¿µ")
            return

        print(f"\nâœ… æ‰¾åˆ° {len(related)} ä¸ªç›¸å…³æ¦‚å¿µ:")
        for i, (related_concept, count) in enumerate(related, 1):
            print(f"   {i:2d}. {related_concept:<15} (å…±ç°{count}æ¬¡)")

    def analyze_concept_evolution(self, concept: str):
        """åˆ†ææ¦‚å¿µæ¼”è¿›"""
        print(f"ğŸ“ˆ åˆ†ææ¦‚å¿µæ¼”è¿›: {concept}")

        evolution = self.concept_analyzer.analyze_concept_evolution(concept)

        if evolution['status'] == 'no_data':
            print(f"âŒ {evolution['message']}")
            return

        overview = evolution['overview']
        print(f"\nâœ… æ¼”è¿›æ¦‚è§ˆ:")
        print(f"   æ—¶é—´è·¨åº¦: {overview['first_year']}-{overview['last_year']} ({overview['span_years']}å¹´)")
        print(f"   æ€»æåŠæ•°: {overview['total_mentions']}")
        print(f"   å³°å€¼å¹´ä»½: {overview['peak_year']} ({overview['peak_count']}ç¯‡)")
        print(f"   æ•´ä½“è¶‹åŠ¿: {overview['trend']}")

        periods = evolution.get('periods', [])
        if periods:
            print(f"\nğŸ“Š åˆ†æœŸåˆ†æ:")
            for period in periods:
                print(f"   {period['period']}: {period['total_mentions']}ç¯‡ "
                      f"(å³°å€¼: {period['peak_year']})")

    def show_system_stats(self):
        """æ˜¾ç¤ºç³»ç»Ÿç»Ÿè®¡"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ç³»ç»Ÿç»Ÿè®¡")
        print("=" * 60)

        # RAGç»Ÿè®¡
        rag_stats = self.rag.get_statistics()

        print(f"\nğŸ” æ£€ç´¢ç³»ç»Ÿ:")
        print(f"  å‘é‡æ•°æ®åº“: {rag_stats['collection_name']}")
        print(f"  æ–‡æ¡£æ€»æ•°: {rag_stats['total_documents']:,}")
        print(f"  åµŒå…¥æ¨¡å‹: {rag_stats['embedding_model']}")
        print(f"  åµŒå…¥ç»´åº¦: {rag_stats['embedding_dim']}")
        print(f"  BM25å¯ç”¨: {'æ˜¯' if rag_stats['bm25_enabled'] else 'å¦'}")

        if rag_stats.get('bm25_indexed'):
            print(f"  BM25æ–‡æ¡£æ•°: {rag_stats.get('bm25_documents', 0):,}")

        # æ¨¡å‹æ€§èƒ½ä¿¡æ¯
        print(f"\nğŸ¤– æ¨¡å‹ä¿¡æ¯:")
        print(f"  è®¾å¤‡: {self.embedding_model.device}")
        print(f"  æ‰¹æ¬¡å¤§å°: {self.embedding_model.batch_size}")
        print(f"  FP16æ¨¡å¼: {'å¼€å¯' if hasattr(self.embedding_model, 'model') and self.embedding_model.device == 'cuda' else 'å…³é—­'}")

        # ä½œè€…ç»Ÿè®¡ï¼ˆå¦‚æœå·²åˆå§‹åŒ–ï¼‰
        if self.author_analyzer:
            author_stats = self.author_analyzer.get_statistics()
            print(f"\nğŸ‘¤ ä½œè€…åˆ†æ:")
            print(f"  ç´¢å¼•çŠ¶æ€: {'å·²æ„å»º' if author_stats.get('index_built', False) else 'æœªæ„å»º'}")
            if author_stats.get('index_built', False):
                print(f"  ä½œè€…æ€»æ•°: {author_stats['total_authors']:,}")
                print(f"  å¹³å‡æ¯ä½œè€…æ–‡æ¡£æ•°: {author_stats['avg_docs_per_author']:.1f}")
                print(f"  æœ€å¤šæ–‡æ¡£ä½œè€…: {author_stats['max_docs_by_author']}ç¯‡")
                print(f"  å¤šç¯‡ä½œè€…æ•°: {author_stats['authors_with_multiple_docs']:,}")
        else:
            print(f"\nğŸ‘¤ ä½œè€…åˆ†æ: æœªåˆå§‹åŒ–")

        # æ¦‚å¿µç»Ÿè®¡ï¼ˆå¦‚æœå·²åˆå§‹åŒ–ï¼‰
        if self.concept_analyzer:
            concept_stats = self.concept_analyzer.get_concept_statistics()
            print(f"\nğŸ’¡ æ¦‚å¿µåˆ†æ:")
            print(f"  é¢„å®šä¹‰æ¦‚å¿µ: {concept_stats['known_concepts_count']}ä¸ª")
            print(f"  ç¼“å­˜å¤§å°: {concept_stats['cache_size']}")
        else:
            print(f"\nğŸ’¡ æ¦‚å¿µåˆ†æ: æœªåˆå§‹åŒ–")

        # æœç´¢å†å²
        if hasattr(self.rag, 'search_history') and self.rag.search_history:
            print(f"\nğŸ“ˆ ä½¿ç”¨ç»Ÿè®¡:")
            print(f"  æ€»æŸ¥è¯¢æ•°: {len(self.rag.search_history)}")

            # ç»Ÿè®¡æŸ¥è¯¢æ–¹æ³•
            methods = {}
            for record in self.rag.search_history:
                method = record.get('method', 'unknown')
                methods[method] = methods.get(method, 0) + 1

            for method, count in methods.items():
                print(f"  {method}: {count}æ¬¡")

    def show_author_list(self):
        """æ˜¾ç¤ºä½œè€…åˆ—è¡¨"""
        print("\nğŸ“– ä½œè€…åˆ—è¡¨ï¼ˆæŒ‰å‘æ–‡æ•°æ’åºï¼Œå‰20ä½ï¼‰:")

        # åˆå§‹åŒ–ä½œè€…åˆ†æå™¨ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
        self.init_author_analyzer()

        author_list = self.author_analyzer.get_author_list(limit=20)

        if not author_list:
            print("âŒ æš‚æ— ä½œè€…æ•°æ®")
            return

        print("-" * 50)
        for i, (author, count) in enumerate(author_list, 1):
            print(f"   {i:2d}. {author:<20} ({count}ç¯‡)")

        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = self.author_analyzer.get_statistics()
        total_authors = stats.get('total_authors', 0)

        if total_authors > 20:
            print(f"\n   ... è¿˜æœ‰ {total_authors - 20:,} ä¸ªä½œè€…")

        print(f"\nğŸ’¡ ä½¿ç”¨ '/author ä½œè€…å' æŸ¥çœ‹å…·ä½“ä½œè€…ä¿¡æ¯")

    def run_tests(self):
        """è¿è¡ŒåŠŸèƒ½æµ‹è¯•"""
        print("\nğŸ§ª è¿è¡Œç³»ç»Ÿæµ‹è¯•...")

        # æµ‹è¯•åŸºç¡€æœç´¢
        print("\n1. æµ‹è¯•åŸºç¡€æœç´¢åŠŸèƒ½:")
        test_query = "è®¾è®¡"
        try:
            results, stats = self.rag.hybrid_search(test_query, top_k=3)
            print(f"   æµ‹è¯•æŸ¥è¯¢: {test_query}")
            print(f"   âœ… æˆåŠŸ - æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            if results:
                print(f"   ğŸ“Š å¹³å‡ç›¸å…³åº¦: {sum(r.get('score', 0) for r in results) / len(results):.3f}")
        except Exception as e:
            print(f"   âŒ å¤±è´¥ - {e}")

        # æµ‹è¯•åµŒå…¥æ¨¡å‹
        print("\n2. æµ‹è¯•åµŒå…¥æ¨¡å‹:")
        try:
            test_text = "åŒ…è±ªæ–¯è®¾è®¡ç†å¿µ"
            embedding = self.embedding_model.encode(test_text)
            print(f"   æµ‹è¯•æ–‡æœ¬: {test_text}")
            print(f"   âœ… æˆåŠŸ - ç»´åº¦: {embedding.shape}")
        except Exception as e:
            print(f"   âŒ å¤±è´¥ - {e}")

        # æµ‹è¯•ä½œè€…æœç´¢ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        print("\n3. æµ‹è¯•ä½œè€…æœç´¢åŠŸèƒ½:")
        try:
            self.init_author_analyzer()
            test_authors = self.author_analyzer.get_author_list(limit=3)

            if test_authors:
                for author, count in test_authors:
                    print(f"   æµ‹è¯•ä½œè€…: {author}")
                    result = self.author_analyzer.analyze_author(author)
                    if result['status'] == 'found':
                        print(f"   âœ… æˆåŠŸ - æ‰¾åˆ° {result['total_publications']} ç¯‡æ–‡ç« ")
                    else:
                        print(f"   âŒ å¤±è´¥ - {result['message']}")
                    break  # åªæµ‹è¯•ä¸€ä¸ªä½œè€…
            else:
                print("   âŒ æ— ä½œè€…æ•°æ®å¯æµ‹è¯•")
        except Exception as e:
            print(f"   âŒ ä½œè€…æµ‹è¯•å¤±è´¥ - {e}")

        # æµ‹è¯•æ¦‚å¿µæœç´¢ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        print("\n4. æµ‹è¯•æ¦‚å¿µæœç´¢åŠŸèƒ½:")
        try:
            self.init_concept_analyzer()
            test_concepts = ['åŒ…è±ªæ–¯', 'ç°ä»£ä¸»ä¹‰', 'è®¾è®¡']

            for concept in test_concepts[:1]:  # åªæµ‹è¯•ä¸€ä¸ªæ¦‚å¿µ
                print(f"   æµ‹è¯•æ¦‚å¿µ: {concept}")
                first_result = self.concept_analyzer.find_first_appearance(concept)
                if first_result['status'] == 'found':
                    print(f"   âœ… æˆåŠŸ - é¦–æ¬¡å‡ºç°: {first_result['year']}")
                else:
                    print(f"   âš ï¸  éƒ¨åˆ†æˆåŠŸ - {first_result.get('message', 'æœªçŸ¥çŠ¶æ€')}")
                break
        except Exception as e:
            print(f"   âŒ æ¦‚å¿µæµ‹è¯•å¤±è´¥ - {e}")

        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    try:
        system = FastRAGInteractiveSystem()
        system.run()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")

        # åœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†é”™è¯¯
        if os.getenv('DEBUG'):
            import traceback
            traceback.print_exc()
        else:
            print("ğŸ’¡ è®¾ç½®ç¯å¢ƒå˜é‡ DEBUG=1 æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯")


if __name__ == "__main__":
    main()