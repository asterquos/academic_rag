"""
test_rag.py

# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python test_rag.py

# è¿è¡Œç‰¹å®šæµ‹è¯•
python test_rag.py --test search
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

import time
import logging
from typing import List, Dict, Tuple
import pandas as pd
import numpy as np

# å¯¼å…¥RAGç»„ä»¶
from src.rag.engine import RAGEngine
from src.rag.embedding import EmbeddingModel
from src.analysis.concept_analyzer import ConceptAnalyzer
from src.analysis.author_analyzer import AuthorAnalyzer

# å¯¼å…¥ç»Ÿä¸€çš„æ¨¡å‹é…ç½®
from model_config import get_embedding_model_config

logger = logging.getLogger(__name__)


class RAGTester:
    """RAGç³»ç»Ÿæµ‹è¯•å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        # ä½¿ç”¨æ–°é…ç½®åˆå§‹åŒ–RAGå¼•æ“
        self.rag = RAGEngine(
            collection_name="art_design_docs_v2",
            persist_directory="data/chroma_v2",
            embedding_model=None,  # å°†å•ç‹¬è®¾ç½®
            enable_bm25=True
        )

        # è®¾ç½®é«˜ç»´åº¦åµŒå…¥æ¨¡å‹ï¼ˆä½¿ç”¨ç»Ÿä¸€é…ç½®ï¼‰
        embedding_kwargs = get_embedding_model_config(
            model_type='bge-large-zh',
            use_fp16=True,
            batch_size=64
        )
        self.rag.embedding_model = EmbeddingModel(**embedding_kwargs)

        logger.info("åˆå§‹åŒ–RAGç³»ç»Ÿ")
        logger.info(f"åµŒå…¥æ¨¡å‹: {self.rag.embedding_model.model_name}")
        logger.info(f"å‘é‡ç»´åº¦: {self.rag.embedding_model.embedding_dim}")

    def test_basic_search(self):
        """æµ‹è¯•åŸºç¡€æœç´¢åŠŸèƒ½"""
        print("\n" + "=" * 70)
        print("1. æµ‹è¯•åŸºç¡€æœç´¢åŠŸèƒ½")
        print("=" * 70)

        test_queries = [
            "åŒ…è±ªæ–¯çš„è®¾è®¡ç†å¿µæ˜¯ä»€ä¹ˆ",
            "ç°ä»£ä¸»ä¹‰è®¾è®¡çš„ç‰¹ç‚¹",
            "å·¥ä¸šè®¾è®¡å’Œå·¥è‰ºç¾æœ¯çš„åŒºåˆ«",
            "ä¸­å›½ä¼ ç»Ÿè®¾è®¡æ–‡åŒ–"
        ]

        for query in test_queries:
            print(f"\næŸ¥è¯¢: {query}")
            start_time = time.time()

            results, stats = self.rag.hybrid_search(query, top_k=5)

            elapsed = time.time() - start_time
            print(f"æ£€ç´¢æ—¶é—´: {elapsed:.3f}ç§’")
            print(f"æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")

            if results:
                # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
                for i, doc in enumerate(results[:3], 1):
                    metadata = doc.get('metadata', {})
                    score = doc.get('rerank_score', doc.get('score', 0))

                    print(f"\n  [{i}] ç›¸å…³åº¦: {score:.3f}")
                    print(f"      æ ‡é¢˜: {metadata.get('æ–‡ç« åç§°+å‰¯æ ‡é¢˜', 'N/A')}")
                    print(f"      ä½œè€…: {metadata.get('ä½œè€…åç§°', 'N/A')}")
                    print(f"      å¹´ä»½: {metadata.get('å¹´ä»½', 'N/A')}")
                    print(f"      ç­–ç•¥: {metadata.get('chunk_strategy', 'N/A')}")
                    print(f"      é¢„è§ˆ: {doc.get('text', '')[:100]}...")

    def test_concept_analysis(self):
        """æµ‹è¯•æ¦‚å¿µåˆ†æåŠŸèƒ½"""
        print("\n" + "=" * 70)
        print("2. æµ‹è¯•æ¦‚å¿µåˆ†æåŠŸèƒ½")
        print("=" * 70)

        analyzer = ConceptAnalyzer(self.rag)

        test_concepts = ["åŒ…è±ªæ–¯", "ç°ä»£ä¸»ä¹‰", "å·¥ä¸šè®¾è®¡"]

        for concept in test_concepts:
            print(f"\nåˆ†ææ¦‚å¿µ: {concept}")

            # æµ‹è¯•é¦–æ¬¡å‡ºç°
            start_time = time.time()
            first_appearance = analyzer.find_first_appearance(concept)
            elapsed = time.time() - start_time

            if first_appearance['status'] == 'found':
                print(f"  é¦–æ¬¡å‡ºç°: {first_appearance['year']}å¹´")
                print(f"  æ–‡çŒ®: ã€Š{first_appearance['title']}ã€‹")
                print(f"  ä½œè€…: {first_appearance['author']}")
                print(f"  åˆ†ææ—¶é—´: {elapsed:.3f}ç§’")
            else:
                print(f"  æœªæ‰¾åˆ°æ¦‚å¿µé¦–æ¬¡å‡ºç°")

            # æµ‹è¯•æ—¶é—´åˆ†å¸ƒ
            temporal_df = analyzer.analyze_temporal_distribution(concept)
            if not temporal_df.empty:
                print(f"  æ—¶é—´åˆ†å¸ƒ: {len(temporal_df)}ä¸ªå¹´ä»½")
                print(f"  æ€»å‡ºç°æ¬¡æ•°: {temporal_df['count'].sum()}")

    # def test_performance_comparison(self):
    #     """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    #     print("\n" + "=" * 70)
    #     print("3. æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    #     print("=" * 70)
    #
    #     # å‡†å¤‡æµ‹è¯•æŸ¥è¯¢
    #     test_queries = [
    #         "åŒ…è±ªæ–¯å­¦æ ¡çš„å»ºç«‹å’Œå½±å“",
    #         "è£…é¥°è‰ºæœ¯è¿åŠ¨çš„ç‰¹å¾",
    #         "æç®€ä¸»ä¹‰è®¾è®¡ç†å¿µ",
    #         "åç°ä»£ä¸»ä¹‰çš„æ‰¹åˆ¤æ€§",
    #         "æ•°å­—åŒ–è®¾è®¡çš„å‘å±•è¶‹åŠ¿"
    #     ]
    #
    #     # æµ‹è¯•ä¸åŒæ£€ç´¢æ–¹æ³•
    #     methods = ['vector', 'bm25', 'hybrid']
    #     results_summary = {method: {'time': [], 'count': [], 'scores': []} for method in methods}
    #
    #
    #     for query in test_queries:
    #         print(f"\næŸ¥è¯¢: {query}")
    #
    #         for method in methods:
    #             start_time = time.time()
    #             results, stats = self.rag.hybrid_search(
    #                 query,
    #                 top_k=10,
    #                 method=method
    #             )
    #             elapsed = time.time() - start_time
    #
    #             # è®°å½•ç»“æœ
    #             results_summary[method]['time'].append(elapsed)
    #             results_summary[method]['count'].append(len(results))
    #             if results:
    #                 avg_score = np.mean([r.get('score', 0) for r in results[:5]])
    #                 results_summary[method]['scores'].append(avg_score)
    #             else:
    #                 results_summary[method]['scores'].append(0)
    #
    #             print(f"  {method}: {elapsed:.3f}ç§’, {len(results)}ä¸ªç»“æœ")
    #
    #     # æ‰“å°æ±‡æ€»
    #     print("\næ€§èƒ½æ±‡æ€»:")
    #     print(f"{'æ–¹æ³•':<10} {'å¹³å‡æ—¶é—´':<10} {'å¹³å‡ç»“æœæ•°':<12} {'å¹³å‡ç›¸å…³åº¦':<10}")
    #     print("-" * 45)
    #
    #     for method in methods:
    #         avg_time = np.mean(results_summary[method]['time'])
    #         avg_count = np.mean(results_summary[method]['count'])
    #         avg_score = np.mean(results_summary[method]['scores'])
    #         print(f"{method:<10} {avg_time:<10.3f} {avg_count:<12.1f} {avg_score:<10.3f}")

    def test_performance_comparison(self):
        """æ€§èƒ½å¯¹æ¯”æµ‹è¯• - ä¿®å¤ç‰ˆæœ¬"""
        print("\n" + "=" * 70)
        print("3. æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
        print("=" * 70)

        # å‡†å¤‡æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "åŒ…è±ªæ–¯å­¦æ ¡çš„å»ºç«‹å’Œå½±å“",
            "è£…é¥°è‰ºæœ¯è¿åŠ¨çš„ç‰¹å¾",
            "æç®€ä¸»ä¹‰è®¾è®¡ç†å¿µ",
            "åç°ä»£ä¸»ä¹‰çš„æ‰¹åˆ¤æ€§",
            "æ•°å­—åŒ–è®¾è®¡çš„å‘å±•è¶‹åŠ¿"
        ]

        # æµ‹è¯•ä¸åŒæ£€ç´¢æ–¹æ³•
        methods = ['vector', 'bm25', 'hybrid']
        results_summary = {method: {'time': [], 'count': [], 'scores': []} for method in methods}

        for query in test_queries:
            print(f"\næŸ¥è¯¢: {query}")

            for method in methods:
                start_time = time.time()
                results, stats = self.rag.hybrid_search(
                    query=query,
                    top_k=10,
                    method=method
                )
                elapsed = time.time() - start_time

                # è®°å½•ç»“æœ
                results_summary[method]['time'].append(elapsed)
                results_summary[method]['count'].append(len(results))

                if results:
                    # ğŸ”§ ä¿®å¤ï¼šæ‰‹åŠ¨å½’ä¸€åŒ–åˆ†æ•°è¿›è¡Œå…¬å¹³æ¯”è¾ƒ
                    raw_scores = []

                    # æå–åŸå§‹åˆ†æ•°
                    for r in results[:5]:
                        if method == 'vector':
                            # å‘é‡æ£€ç´¢ï¼šä½¿ç”¨distanceè½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼Œæˆ–ç›´æ¥ä½¿ç”¨score
                            if 'distance' in r:
                                score = 1 - r['distance']  # distanceè¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜
                            else:
                                score = r.get('score', 0)
                        elif method == 'bm25':
                            # BM25æ£€ç´¢ï¼šä½¿ç”¨åŸå§‹BM25åˆ†æ•°
                            score = r.get('score', 0)
                        else:  # hybrid
                            # æ··åˆæ£€ç´¢ï¼šä¼˜å…ˆä½¿ç”¨combined_score
                            score = r.get('combined_score', r.get('rerank_score', r.get('score', 0)))

                        raw_scores.append(score)

                    # æ‰‹åŠ¨å½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´
                    if raw_scores:
                        min_score = min(raw_scores)
                        max_score = max(raw_scores)

                        if max_score > min_score:
                            # Min-Maxå½’ä¸€åŒ–
                            normalized_scores = [(s - min_score) / (max_score - min_score) for s in raw_scores]
                        else:
                            # æ‰€æœ‰åˆ†æ•°ç›¸åŒçš„æƒ…å†µ
                            normalized_scores = [0.5] * len(raw_scores)

                        avg_score = np.mean(normalized_scores)
                    else:
                        avg_score = 0

                    results_summary[method]['scores'].append(avg_score)

                    # è°ƒè¯•è¾“å‡ºï¼šæ˜¾ç¤ºåŸå§‹åˆ†æ•°èŒƒå›´
                    if raw_scores:
                        print(f"    {method}: {elapsed:.3f}ç§’, {len(results)}ä¸ªç»“æœ "
                              f"(åˆ†æ•°èŒƒå›´: {min(raw_scores):.3f}-{max(raw_scores):.3f})")
                    else:
                        print(f"    {method}: {elapsed:.3f}ç§’, {len(results)}ä¸ªç»“æœ")
                else:
                    results_summary[method]['scores'].append(0)
                    print(f"    {method}: {elapsed:.3f}ç§’, 0ä¸ªç»“æœ")

        # æ‰“å°æ±‡æ€»ç»Ÿè®¡
        print("\næ€§èƒ½æ±‡æ€»:")
        print(f"{'æ–¹æ³•':<10} {'å¹³å‡æ—¶é—´':<10} {'å¹³å‡ç»“æœæ•°':<12} {'å¹³å‡ç›¸å…³åº¦':<12} {'æ—¶é—´æ’å':<8}")
        print("-" * 55)

        # è®¡ç®—æ’å
        avg_times = {method: np.mean(results_summary[method]['time']) for method in methods}
        time_ranking = sorted(avg_times.items(), key=lambda x: x[1])
        time_ranks = {method: i + 1 for i, (method, _) in enumerate(time_ranking)}

        for method in methods:
            avg_time = np.mean(results_summary[method]['time'])
            avg_count = np.mean(results_summary[method]['count'])
            avg_score = np.mean(results_summary[method]['scores'])
            time_rank = time_ranks[method]

            print(f"{method:<10} {avg_time:<10.3f} {avg_count:<12.1f} {avg_score:<12.3f} #{time_rank}")

        # é¢å¤–çš„åˆ†æ
        print(f"\nğŸ“Š è¯¦ç»†åˆ†æ:")

        # æœ€å¿«çš„æ–¹æ³•
        fastest_method = min(avg_times.items(), key=lambda x: x[1])
        print(f"âš¡ æœ€å¿«æ£€ç´¢: {fastest_method[0]} ({fastest_method[1]:.3f}ç§’)")

        # æœ€é«˜ç›¸å…³åº¦çš„æ–¹æ³•
        avg_relevance = {method: np.mean(results_summary[method]['scores']) for method in methods}
        best_relevance = max(avg_relevance.items(), key=lambda x: x[1])
        print(f"ğŸ¯ æœ€é«˜ç›¸å…³åº¦: {best_relevance[0]} ({best_relevance[1]:.3f})")

        # æ··åˆæ£€ç´¢çš„æ•ˆç‡åˆ†æ
        if 'hybrid' in avg_times and 'vector' in avg_times:
            hybrid_overhead = avg_times['hybrid'] - avg_times['vector']
            print(f"ğŸ”„ æ··åˆæ£€ç´¢å¼€é”€: +{hybrid_overhead:.3f}ç§’ ({hybrid_overhead / avg_times['vector'] * 100:.1f}%)")

        # åˆ†æ•°ä¸€è‡´æ€§æ£€æŸ¥
        print(f"\nğŸ” åˆ†æ•°åˆ†å¸ƒæ£€æŸ¥:")
        for method in methods:
            scores = results_summary[method]['scores']
            if scores:
                std_dev = np.std(scores)
                print(
                    f"  {method}: æ ‡å‡†å·®={std_dev:.3f} (ä¸€è‡´æ€§: {'é«˜' if std_dev < 0.1 else 'ä¸­' if std_dev < 0.2 else 'ä½'})")

    def test_chunk_strategies(self):
        """æµ‹è¯•ä¸åŒchunkç­–ç•¥çš„æ•ˆæœ"""
        print("\n" + "=" * 70)
        print("4. æµ‹è¯•Chunkç­–ç•¥æ•ˆæœ")
        print("=" * 70)

        # è¿™ä¸ªæµ‹è¯•éœ€è¦åˆ†åˆ«ç”¨ä¸åŒç­–ç•¥æ„å»ºçš„æ•°æ®
        # è¿™é‡Œå±•ç¤ºå¦‚ä½•åˆ†æå½“å‰ç­–ç•¥çš„æ•ˆæœ

        test_query = "åŒ…è±ªæ–¯çš„æ•™è‚²ç†å¿µå’Œæ–¹æ³•"
        results, _ = self.rag.hybrid_search(test_query, top_k=10)

        # ç»Ÿè®¡ä¸åŒç­–ç•¥çš„ç»“æœ
        strategy_counts = {}
        for doc in results:
            strategy = doc.get('metadata', {}).get('chunk_strategy', 'unknown')
            strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1

        print(f"æŸ¥è¯¢: {test_query}")
        print(f"ç»“æœä¸­çš„chunkç­–ç•¥åˆ†å¸ƒ:")
        for strategy, count in strategy_counts.items():
            print(f"  {strategy}: {count}ä¸ª")

        # åˆ†æchunké•¿åº¦ä¸ç›¸å…³åº¦çš„å…³ç³»
        if results:
            lengths = []
            scores = []
            for doc in results:
                length = int(doc.get('metadata', {}).get('chunk_length', 0))
                # ä½¿ç”¨é€‚å½“çš„åˆ†æ•°å­—æ®µ
                score = doc.get('rerank_score',
                                doc.get('combined_score', doc.get('normalized_score', doc.get('score', 0))))
                lengths.append(length)
                scores.append(score)

            if lengths and scores:
                correlation = np.corrcoef(lengths, scores)[0, 1]
                print(f"\nChunké•¿åº¦ä¸ç›¸å…³åº¦çš„ç›¸å…³ç³»æ•°: {correlation:.3f}")
                print(f"å¹³å‡chunké•¿åº¦: {np.mean(lengths):.0f}å­—ç¬¦")
                print(f"å¹³å‡ç›¸å…³åº¦åˆ†æ•°: {np.mean(scores):.3f}")

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "=" * 70)
        print("å¼€å§‹æµ‹è¯•RAGç³»ç»Ÿ")
        print("=" * 70)

        # æ£€æŸ¥å‘é‡åº“çŠ¶æ€
        doc_count = self.rag.vector_store.count()
        print(f"\nå‘é‡åº“æ–‡æ¡£æ•°: {doc_count}")

        if doc_count == 0:
            print("âŒ å‘é‡åº“ä¸ºç©ºï¼Œè¯·å…ˆè¿è¡Œ build_rag.py æ„å»ºæ•°æ®")
            return

        # è¿è¡Œå„é¡¹æµ‹è¯•
        self.test_basic_search()
        self.test_concept_analysis()
        self.test_performance_comparison()
        self.test_chunk_strategies()

        print("\n" + "=" * 70)
        print("âœ… æµ‹è¯•å®Œæˆï¼")
        print("=" * 70)


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='æµ‹è¯•RAGç³»ç»Ÿ')
    parser.add_argument('--test', type=str, default='all',
                        choices=['all', 'search', 'concept', 'performance', 'chunk'],
                        help='è¦è¿è¡Œçš„æµ‹è¯•ç±»å‹')

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = RAGTester()

    # è¿è¡Œæµ‹è¯•
    if args.test == 'all':
        tester.run_all_tests()
    elif args.test == 'search':
        tester.test_basic_search()
    elif args.test == 'concept':
        tester.test_concept_analysis()
    elif args.test == 'performance':
        tester.test_performance_comparison()
    elif args.test == 'chunk':
        tester.test_chunk_strategies()


if __name__ == "__main__":
    main()