"""
test_preprocessing.py
æ•°æ®é¢„å¤„ç†æ¨¡å—ä¸“é¡¹æµ‹è¯•
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json
import time
from datetime import datetime
import logging
import matplotlib.pyplot as plt
import seaborn as sns

from src.preprocessing.excel_parser import ExcelParser
from src.preprocessing.text_processor import TextProcessor
from src.preprocessing.data_validator import DataValidator
from src.preprocessing.pipeline import PreprocessingPipeline

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False


class PreprocessingTester:
    """æ•°æ®é¢„å¤„ç†æµ‹è¯•å™¨"""
    
    def __init__(self, config_path="config.yaml"):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        import yaml
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
            
        self.results = {
            'timestamp': datetime.now().isoformat(),
            'tests': {}
        }
        
    def test_excel_parser(self):
        """æµ‹è¯•Excelè§£æå™¨"""
        print("\n" + "="*60)
        print("æµ‹è¯• Excel è§£æå™¨")
        print("="*60)
        
        parser = ExcelParser()
        results = {'status': 'testing', 'details': {}}
        
        try:
            # 1. æµ‹è¯•åŠ è½½
            print("\n1. æµ‹è¯•ExcelåŠ è½½...")
            start_time = time.time()
            df = parser.load_excel(self.config['data']['raw_path'])
            load_time = time.time() - start_time
            
            results['details']['load_time'] = f"{load_time:.2f}ç§’"
            results['details']['shape'] = df.shape
            results['details']['columns'] = list(df.columns)
            
            print(f"âœ“ åŠ è½½æˆåŠŸ")
            print(f"  - è€—æ—¶: {load_time:.2f}ç§’")
            print(f"  - æ•°æ®é‡: {df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—")
            
            # 2. æµ‹è¯•åˆ—éªŒè¯
            print("\n2. æµ‹è¯•åˆ—éªŒè¯...")
            valid, missing = parser.validate_columns(df)
            results['details']['columns_valid'] = valid
            results['details']['missing_columns'] = missing
            
            if valid:
                print("âœ“ æ‰€æœ‰å¿…éœ€åˆ—éƒ½å­˜åœ¨")
            else:
                print(f"âœ— ç¼ºå¤±åˆ—: {missing}")
                
            # 3. æµ‹è¯•å¹´ä»½å¡«å……
            print("\n3. æµ‹è¯•å¹´ä»½å¡«å……...")
            year_nulls_before = df['å¹´ä»½'].isna().sum()
            df_filled = parser.fill_year_values(df)
            year_nulls_after = df_filled['å¹´ä»½'].isna().sum()
            
            results['details']['year_fill'] = {
                'before': int(year_nulls_before),
                'after': int(year_nulls_after),
                'filled': int(year_nulls_before - year_nulls_after)
            }
            
            print(f"âœ“ å¹´ä»½å¡«å……å®Œæˆ")
            print(f"  - å¡«å……å‰ç©ºå€¼: {year_nulls_before}")
            print(f"  - å¡«å……åç©ºå€¼: {year_nulls_after}")
            print(f"  - å¡«å……æ•°é‡: {year_nulls_before - year_nulls_after}")
            
            # 4. æµ‹è¯•å®Œæ•´å¤„ç†æµç¨‹
            print("\n4. æµ‹è¯•å®Œæ•´å¤„ç†æµç¨‹...")
            start_time = time.time()
            processed_df = parser.process(self.config['data']['raw_path'])
            process_time = time.time() - start_time
            
            results['details']['process_time'] = f"{process_time:.2f}ç§’"
            results['details']['processed_shape'] = processed_df.shape
            
            print(f"âœ“ å¤„ç†å®Œæˆ")
            print(f"  - è€—æ—¶: {process_time:.2f}ç§’")
            print(f"  - è¾“å‡ºæ•°æ®: {processed_df.shape[0]}è¡Œ")
            
            # 5. æ•°æ®è´¨é‡æ£€æŸ¥
            print("\n5. æ•°æ®è´¨é‡æ£€æŸ¥...")
            quality_checks = {
                'æ ‡é¢˜ä¸ºç©º': processed_df['æ–‡ç« åç§°+å‰¯æ ‡é¢˜'].isna().sum(),
                'ä½œè€…ä¸ºç©º': processed_df['ä½œè€…åç§°'].isna().sum(),
                'å…¨æ–‡ä¸ºç©º': processed_df['å…¨æ–‡'].isna().sum(),
                'å¹´ä»½ä¸ºç©º': processed_df['å¹´ä»½'].isna().sum()
            }
            
            results['details']['quality_checks'] = quality_checks
            
            for check, count in quality_checks.items():
                status = "âœ“" if count == 0 else "âš ï¸"
                print(f"  {status} {check}: {count}")
                
            results['status'] = 'passed'
            results['processed_df'] = processed_df
            
        except Exception as e:
            results['status'] = 'failed'
            results['error'] = str(e)
            print(f"\nâœ— æµ‹è¯•å¤±è´¥: {e}")
            
        self.results['tests']['excel_parser'] = results
        return results.get('processed_df')
        
    def test_text_processor(self, df=None):
        """æµ‹è¯•æ–‡æœ¬å¤„ç†å™¨"""
        print("\n" + "="*60)
        print("æµ‹è¯•æ–‡æœ¬å¤„ç†å™¨")
        print("="*60)
        
        if df is None:
            print("âš ï¸  æ²¡æœ‰è¾“å…¥æ•°æ®ï¼Œè·³è¿‡æµ‹è¯•")
            return None
            
        processor = TextProcessor()
        results = {'status': 'testing', 'details': {}}
        
        try:
            # 1. æµ‹è¯•æ–‡æœ¬æ¸…æ´—
            print("\n1. æµ‹è¯•æ–‡æœ¬æ¸…æ´—...")
            sample_text = df['å…¨æ–‡'].iloc[0] if len(df) > 0 else ""
            cleaned_text = processor.clean_text(sample_text)
            
            results['details']['text_cleaning'] = {
                'original_length': len(sample_text),
                'cleaned_length': len(cleaned_text),
                'reduction': f"{(1 - len(cleaned_text)/len(sample_text))*100:.1f}%"
            }
            
            print(f"âœ“ æ–‡æœ¬æ¸…æ´—æµ‹è¯•å®Œæˆ")
            print(f"  - åŸå§‹é•¿åº¦: {len(sample_text)}")
            print(f"  - æ¸…æ´—åé•¿åº¦: {len(cleaned_text)}")
            
            # 2. æµ‹è¯•åˆ†è¯
            print("\n2. æµ‹è¯•åˆ†è¯åŠŸèƒ½...")
            words = processor.segment_text(cleaned_text[:200])
            results['details']['segmentation'] = {
                'sample_words': words[:10],
                'word_count': len(words)
            }
            
            print(f"âœ“ åˆ†è¯æµ‹è¯•å®Œæˆ")
            print(f"  - è¯æ•°: {len(words)}")
            print(f"  - ç¤ºä¾‹: {' | '.join(words[:10])}")
            
            # 3. æµ‹è¯•æ¦‚å¿µæå–
            print("\n3. æµ‹è¯•æ¦‚å¿µæå–...")
            concepts = processor.extract_concepts(cleaned_text)
            results['details']['concepts'] = {
                'count': len(concepts),
                'top_concepts': list(concepts.items())[:5]
            }
            
            print(f"âœ“ æ¦‚å¿µæå–å®Œæˆ")
            print(f"  - æ¦‚å¿µæ•°: {len(concepts)}")
            if concepts:
                print(f"  - Top 5: {list(concepts.keys())[:5]}")
                
            # 4. æµ‹è¯•æ–‡æœ¬åˆ†å—
            print("\n4. æµ‹è¯•æ–‡æœ¬åˆ†å—...")
            chunks = processor.create_text_chunks(cleaned_text, chunk_size=500, overlap=50)
            results['details']['chunking'] = {
                'chunk_count': len(chunks),
                'avg_chunk_size': np.mean([len(c['text']) for c in chunks]) if chunks else 0
            }
            
            print(f"âœ“ æ–‡æœ¬åˆ†å—å®Œæˆ")
            print(f"  - å—æ•°: {len(chunks)}")
            print(f"  - å¹³å‡å—å¤§å°: {results['details']['chunking']['avg_chunk_size']:.0f}å­—ç¬¦")
            
            # 5. æ‰¹é‡å¤„ç†æµ‹è¯•
            print("\n5. æµ‹è¯•æ‰¹é‡å¤„ç†...")
            sample_df = df.head(10).copy()
            start_time = time.time()
            processed_df = processor.process_dataframe(sample_df)
            process_time = time.time() - start_time
            
            results['details']['batch_processing'] = {
                'sample_size': len(sample_df),
                'process_time': f"{process_time:.2f}ç§’",
                'new_columns': [col for col in processed_df.columns if col not in sample_df.columns]
            }
            
            print(f"âœ“ æ‰¹é‡å¤„ç†å®Œæˆ")
            print(f"  - å¤„ç†{len(sample_df)}æ¡æ•°æ®è€—æ—¶: {process_time:.2f}ç§’")
            print(f"  - æ–°å¢åˆ—: {results['details']['batch_processing']['new_columns']}")
            
            results['status'] = 'passed'
            
        except Exception as e:
            results['status'] = 'failed'
            results['error'] = str(e)
            print(f"\nâœ— æµ‹è¯•å¤±è´¥: {e}")
            
        self.results['tests']['text_processor'] = results
        return results
        
    def test_data_validator(self, df=None):
        """æµ‹è¯•æ•°æ®éªŒè¯å™¨"""
        print("\n" + "="*60)
        print("æµ‹è¯•æ•°æ®éªŒè¯å™¨")
        print("="*60)
        
        if df is None:
            print("âš ï¸  æ²¡æœ‰è¾“å…¥æ•°æ®ï¼Œè·³è¿‡æµ‹è¯•")
            return None
            
        validator = DataValidator()
        results = {'status': 'testing', 'details': {}}
        
        try:
            print("\næ‰§è¡Œæ•°æ®éªŒè¯...")
            start_time = time.time()
            report = validator.validate_dataframe(df)
            validate_time = time.time() - start_time
            
            results['details']['validation_time'] = f"{validate_time:.2f}ç§’"
            results['details']['quality_score'] = report['data_quality_score']
            results['details']['errors'] = len(report['errors'])
            results['details']['warnings'] = len(report['warnings'])
            
            print(f"\nâœ“ éªŒè¯å®Œæˆ")
            print(f"  - è€—æ—¶: {validate_time:.2f}ç§’")
            print(f"  - æ•°æ®è´¨é‡å¾—åˆ†: {report['data_quality_score']:.1f}/100")
            print(f"  - é”™è¯¯æ•°: {len(report['errors'])}")
            print(f"  - è­¦å‘Šæ•°: {len(report['warnings'])}")
            
            # æ˜¾ç¤ºå…·ä½“é—®é¢˜
            if report['errors']:
                print(f"\né”™è¯¯:")
                for error in report['errors'][:3]:
                    print(f"  - {error}")
                    
            if report['warnings']:
                print(f"\nè­¦å‘Š:")
                for warning in report['warnings'][:3]:
                    print(f"  - {warning}")
                    
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if 'statistics' in report:
                print(f"\nç»Ÿè®¡ä¿¡æ¯:")
                stats = report['statistics']
                
                if 'null_counts' in stats:
                    print(f"  ç©ºå€¼ç»Ÿè®¡:")
                    for field, count in stats['null_counts'].items():
                        print(f"    - {field}: {count}")
                        
                if 'duplicates' in stats:
                    print(f"  é‡å¤æ•°æ®:")
                    for key, value in stats['duplicates'].items():
                        print(f"    - {key}: {value}")
                        
            results['status'] = 'passed'
            results['validation_report'] = report
            
        except Exception as e:
            results['status'] = 'failed'
            results['error'] = str(e)
            print(f"\nâœ— æµ‹è¯•å¤±è´¥: {e}")
            
        self.results['tests']['data_validator'] = results
        return results
        
    def test_pipeline(self):
        """æµ‹è¯•å®Œæ•´é¢„å¤„ç†ç®¡é“"""
        print("\n" + "="*60)
        print("æµ‹è¯•å®Œæ•´é¢„å¤„ç†ç®¡é“")
        print("="*60)
        
        results = {'status': 'testing', 'details': {}}
        
        try:
            pipeline = PreprocessingPipeline()
            
            print("\nè¿è¡Œé¢„å¤„ç†ç®¡é“...")
            start_time = time.time()
            pipeline_results = pipeline.run(self.config['data']['raw_path'])
            total_time = time.time() - start_time
            
            results['details']['total_time'] = f"{total_time:.2f}ç§’"
            results['details']['status'] = pipeline_results['status']
            
            if pipeline_results['status'] == 'completed':
                print(f"\nâœ“ ç®¡é“æ‰§è¡ŒæˆåŠŸ")
                print(f"  - æ€»è€—æ—¶: {total_time:.2f}ç§’")
                print(f"  - æ‰§è¡Œæ—¶é—´: {pipeline_results['duration']}")
                
                # æ˜¾ç¤ºå„æ­¥éª¤ç»“æœ
                print(f"\nå„æ­¥éª¤æ‰§è¡Œç»“æœ:")
                for step, info in pipeline_results['steps'].items():
                    print(f"  - {step}: {info['status']}")
                    
                # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
                output_dir = Path(self.config['preprocessing']['pipeline']['output_dir'])
                output_files = list(output_dir.glob('*.parquet')) + list(output_dir.glob('*.json'))
                
                results['details']['output_files'] = [str(f.name) for f in output_files]
                
                print(f"\nè¾“å‡ºæ–‡ä»¶:")
                for f in output_files:
                    size_mb = f.stat().st_size / 1024 / 1024
                    print(f"  - {f.name} ({size_mb:.2f} MB)")
                    
                results['status'] = 'passed'
                
            else:
                results['status'] = 'failed'
                results['details']['error'] = pipeline_results.get('error')
                print(f"\nâœ— ç®¡é“æ‰§è¡Œå¤±è´¥: {pipeline_results.get('error')}")
                
        except Exception as e:
            results['status'] = 'failed'
            results['error'] = str(e)
            print(f"\nâœ— æµ‹è¯•å¤±è´¥: {e}")
            
        self.results['tests']['pipeline'] = results
        return results
        
    def visualize_results(self, df):
        """å¯è§†åŒ–æ•°æ®åˆ†æç»“æœ"""
        print("\n" + "="*60)
        print("ç”Ÿæˆæ•°æ®å¯è§†åŒ–")
        print("="*60)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('æ•°æ®é¢„å¤„ç†ç»“æœåˆ†æ', fontsize=16)
        
        # 1. å¹´ä»½åˆ†å¸ƒ
        ax1 = axes[0, 0]
        year_counts = df['å¹´ä»½'].value_counts().sort_index()
        ax1.plot(year_counts.index, year_counts.values, marker='o')
        ax1.set_title('æ–‡æ¡£å¹´ä»½åˆ†å¸ƒ')
        ax1.set_xlabel('å¹´ä»½')
        ax1.set_ylabel('æ–‡æ¡£æ•°é‡')
        ax1.grid(True, alpha=0.3)
        
        # 2. æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
        ax2 = axes[0, 1]
        text_lengths = df['å…¨æ–‡é•¿åº¦'].dropna()
        ax2.hist(text_lengths, bins=50, edgecolor='black', alpha=0.7)
        ax2.set_title('æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ')
        ax2.set_xlabel('å­—ç¬¦æ•°')
        ax2.set_ylabel('é¢‘ç‡')
        ax2.axvline(text_lengths.mean(), color='red', linestyle='--', label=f'å¹³å‡: {text_lengths.mean():.0f}')
        ax2.legend()
        
        # 3. åˆ†ç±»åˆ†å¸ƒï¼ˆå‰10ä¸ªï¼‰
        ax3 = axes[1, 0]
        category_counts = df['åˆ†ç±»'].value_counts().head(10)
        ax3.barh(category_counts.index, category_counts.values)
        ax3.set_title('æ–‡æ¡£åˆ†ç±»åˆ†å¸ƒ (Top 10)')
        ax3.set_xlabel('æ–‡æ¡£æ•°é‡')
        
        # 4. æ•°æ®å®Œæ•´æ€§
        ax4 = axes[1, 1]
        null_counts = df.isnull().sum()
        important_fields = ['å¹´ä»½', 'æ–‡ç« åç§°+å‰¯æ ‡é¢˜', 'ä½œè€…åç§°', 'å…¨æ–‡', 'åˆ†ç±»']
        null_data = null_counts[important_fields]
        colors = ['green' if v == 0 else 'orange' if v < 10 else 'red' for v in null_data.values]
        ax4.bar(null_data.index, null_data.values, color=colors)
        ax4.set_title('é‡è¦å­—æ®µç¼ºå¤±å€¼ç»Ÿè®¡')
        ax4.set_ylabel('ç¼ºå¤±å€¼æ•°é‡')
        ax4.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        output_path = Path('data/processed/preprocessing_analysis.png')
        output_path.parent.mkdir(exist_ok=True)
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"\nâœ“ å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        plt.close()
        
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š")
        print("="*60)
        
        # æ±‡æ€»ç»“æœ
        summary = {
            'total_tests': len(self.results['tests']),
            'passed': sum(1 for t in self.results['tests'].values() if t.get('status') == 'passed'),
            'failed': sum(1 for t in self.results['tests'].values() if t.get('status') == 'failed'),
            'timestamp': self.results['timestamp']
        }
        
        self.results['summary'] = summary
        
        # ä¿å­˜JSONæŠ¥å‘Š
        report_path = Path('data/processed/preprocessing_test_report.json')
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
            
        print(f"\næµ‹è¯•æŠ¥å‘Šæ‘˜è¦:")
        print(f"  - æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"  - é€šè¿‡: {summary['passed']}")
        print(f"  - å¤±è´¥: {summary['failed']}")
        print(f"  - æˆåŠŸç‡: {summary['passed']/summary['total_tests']*100:.1f}%")
        print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        return self.results


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\nğŸ”§ æ•°æ®é¢„å¤„ç†æ¨¡å—ä¸“é¡¹æµ‹è¯•")
    print("="*60)
    
    tester = PreprocessingTester()
    
    # 1. æµ‹è¯•Excelè§£æ
    df = tester.test_excel_parser()
    
    # 2. æµ‹è¯•æ–‡æœ¬å¤„ç†
    if df is not None:
        tester.test_text_processor(df)
        
    # 3. æµ‹è¯•æ•°æ®éªŒè¯
    if df is not None:
        tester.test_data_validator(df)
        
    # 4. æµ‹è¯•å®Œæ•´ç®¡é“
    tester.test_pipeline()
    
    # 5. ç”Ÿæˆå¯è§†åŒ–ï¼ˆå¦‚æœæœ‰æ•°æ®ï¼‰
    if df is not None:
        tester.visualize_results(df)
        
    # 6. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    report = tester.generate_report()
    
    print("\n" + "="*60)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("="*60)
    
    # å»ºè®®åç»­æ­¥éª¤
    print("\nå»ºè®®åç»­æ­¥éª¤:")
    print("1. æŸ¥çœ‹ data/processed/ ç›®å½•ä¸‹çš„è¾“å‡ºæ–‡ä»¶")
    print("2. æ£€æŸ¥ preprocessing_test_report.json äº†è§£è¯¦ç»†ç»“æœ")
    print("3. æŸ¥çœ‹ preprocessing_analysis.png äº†è§£æ•°æ®åˆ†å¸ƒ")
    
    if report['summary']['failed'] > 0:
        print("4. âš ï¸  æœ‰æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤")
    else:
        print("4. âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥ç»§ç»­è¿›è¡ŒRAGç´¢å¼•")


if __name__ == "__main__":
    main()