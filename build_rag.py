"""
build_rag.py
æ„å»ºRAGç³»ç»Ÿ - æ•°æ®é¢„å¤„ç†å’Œå‘é‡åŒ–

# ä½¿ç”¨é»˜è®¤é…ç½®
python build_rag.py

# é‡å»ºå‘é‡åº“
python build_rag.py --rebuild

# æŒ‡å®šé…ç½®æ–‡ä»¶
python build_rag.py --config config.yaml
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

import pandas as pd
import numpy as np
from typing import List, Dict, Optional
import logging
from tqdm import tqdm
import time
import json
import yaml
import argparse
import chromadb
from chromadb.config import Settings

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from src.preprocessing.text_processor import TextProcessor, ChunkStrategy
from src.rag.embedding import EmbeddingModel
from src.preprocessing.excel_parser import ExcelParser
from src.preprocessing.data_validator import DataValidator

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class RAGBuilder:
    """RAGç³»ç»Ÿæ„å»ºå™¨"""

    def __init__(self, config_path: str = "config.yaml"):
        """åˆå§‹åŒ–æ„å»ºå™¨"""
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        # åˆå§‹åŒ–ç»„ä»¶
        self.excel_parser = ExcelParser()
        self.text_processor = TextProcessor()
        self.data_validator = DataValidator()

        # è®¾ç½®é»˜è®¤ä½¿ç”¨full_articleç­–ç•¥
        self.text_processor.set_strategy('full_article')
        logger.info("ä½¿ç”¨chunkç­–ç•¥: full_article")

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆä½¿ç”¨é«˜ç»´åº¦æ¨¡å‹ï¼‰
        embedding_config = self.config['embedding']
        model_type = embedding_config.get('current_model', 'bge_large')
        model_config = embedding_config['models'][model_type]

        self.embedding_model = EmbeddingModel(
            model_name=model_config['model_name'],
            device=model_config.get('device', 'cuda'),
            use_fp16=embedding_config.get('mixed_precision', True),
            batch_size=embedding_config.get('batch_size', 64)
        )

        logger.info(f"ä½¿ç”¨åµŒå…¥æ¨¡å‹: {model_config['model_name']} ({model_config['dimension']}ç»´)")

        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.vector_store = self._init_vector_store()

    def _init_vector_store(self):
        """åˆå§‹åŒ–ChromaDBå‘é‡å­˜å‚¨"""
        persist_directory = self.config['vector_store']['persist_directory']
        collection_name = self.config['vector_store']['collection_name']

        # åˆ›å»ºç›®å½•
        Path(persist_directory).mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºChromaDBå®¢æˆ·ç«¯
        self.chroma_client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(anonymized_telemetry=False)
        )

        # å¤„ç†é›†åˆçš„åˆ›å»ºæˆ–é‡å»º
        try:
            if hasattr(self, 'rebuild') and self.rebuild:
                # å¦‚æœéœ€è¦é‡å»ºï¼Œå…ˆå°è¯•åˆ é™¤æ—§é›†åˆ
                try:
                    self.chroma_client.delete_collection(collection_name)
                    logger.info(f"âœ“ åˆ é™¤æ—§é›†åˆ: {collection_name}")
                except Exception as e:
                    logger.warning(f"åˆ é™¤é›†åˆå¤±è´¥ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰: {e}")

                # åˆ›å»ºæ–°é›†åˆ
                collection = self.chroma_client.create_collection(
                    name=collection_name,
                    metadata={"description": "è‰ºæœ¯è®¾è®¡æ–‡çŒ®å‘é‡åº“"}
                )
                logger.info(f"âœ“ åˆ›å»ºæ–°é›†åˆ: {collection_name}")
            else:
                # ä¸é‡å»ºæ—¶ï¼Œå°è¯•è·å–ç°æœ‰é›†åˆ
                try:
                    collection = self.chroma_client.get_collection(collection_name)
                    logger.info(f"âœ“ ä½¿ç”¨ç°æœ‰é›†åˆ: {collection_name} (åŒ…å« {collection.count()} ä¸ªæ–‡æ¡£)")
                except Exception:
                    # å¦‚æœé›†åˆä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
                    collection = self.chroma_client.create_collection(
                        name=collection_name,
                        metadata={"description": "è‰ºæœ¯è®¾è®¡æ–‡çŒ®å‘é‡åº“"}
                    )
                    logger.info(f"âœ“ åˆ›å»ºæ–°é›†åˆ: {collection_name}")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            raise

        return collection

    def process_data(self, input_file: str, output_dir: str = "data/processed"):
        """
        å¤„ç†æ•°æ®çš„ä¸»æµç¨‹

        Args:
            input_file: è¾“å…¥çš„Excelæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        """
        logger.info("=" * 70)
        logger.info("å¼€å§‹æ•°æ®å¤„ç†æµç¨‹")
        logger.info("=" * 70)

        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # Step 1: åŠ è½½å’Œè§£æExcelæ•°æ®
        logger.info("\n[Step 1/6] åŠ è½½Excelæ•°æ®...")
        df = self.excel_parser.process(input_file, filter_selected=False)
        logger.info(f"âœ“ åŠ è½½ {len(df)} æ¡è®°å½•")

        # Step 2: æ•°æ®éªŒè¯
        logger.info("\n[Step 2/6] éªŒè¯æ•°æ®è´¨é‡...")
        validation_report = self.data_validator.validate_dataframe(df)
        logger.info(f"âœ“ æ•°æ®è´¨é‡å¾—åˆ†: {validation_report['data_quality_score']:.1f}/100")

        # ä¿å­˜éªŒè¯æŠ¥å‘Š
        with open(output_path / "validation_report.json", 'w', encoding='utf-8') as f:
            json.dump(validation_report, f, ensure_ascii=False, indent=2)

        # Step 3: åˆ›å»ºæ–‡æœ¬chunks
        logger.info("\n[Step 3/6] åˆ›å»ºæ–‡æœ¬chunks...")
        chunks_data = self._create_chunks(df)
        logger.info(f"âœ“ åˆ›å»º {len(chunks_data)} ä¸ªchunks")

        # Step 4: ç”ŸæˆåµŒå…¥å‘é‡
        logger.info("\n[Step 4/6] ç”ŸæˆåµŒå…¥å‘é‡...")
        embeddings = self._generate_embeddings(chunks_data)
        logger.info(f"âœ“ ç”Ÿæˆ {len(embeddings)} ä¸ªåµŒå…¥å‘é‡ï¼Œç»´åº¦: {embeddings[0].shape}")

        # Step 5: å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        logger.info("\n[Step 5/6] å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“...")
        self._store_to_vectordb(chunks_data, embeddings)

        # Step 6: ä¿å­˜å¤„ç†ç»“æœ
        logger.info("\n[Step 6/6] ä¿å­˜å¤„ç†ç»“æœ...")
        self._save_processed_data(df, chunks_data, output_path)

        logger.info("\n" + "=" * 70)
        logger.info("âœ… æ•°æ®å¤„ç†å®Œæˆï¼")
        logger.info("=" * 70)

        # Step 7: æ„å»ºBM25ç´¢å¼•
        logger.info("\n[Step 7/7] æ„å»ºBM25ç´¢å¼•...")
        self._build_bm25_index(chunks_data)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self._print_statistics(df, chunks_data)

    def _create_chunks(self, df: pd.DataFrame) -> List[Dict]:
        """åˆ›å»ºæ–‡æœ¬chunks"""
        chunks_data = []

        for idx, row in tqdm(df.iterrows(), total=len(df), desc="åˆ›å»ºchunks"):
            # æ„å»ºæ–‡æ¡£å¯¹è±¡
            doc = {
                'doc_id': row.get('doc_id', f"doc_{idx}"),
                'text': row.get('å…¨æ–‡', ''),
                'metadata': {
                    'å¹´ä»½': row.get('å¹´ä»½'),
                    'ä½œè€…åç§°': row.get('ä½œè€…åç§°'),
                    'æ–‡ç« åç§°+å‰¯æ ‡é¢˜': row.get('æ–‡ç« åç§°+å‰¯æ ‡é¢˜'),
                    'åˆ†ç±»': row.get('åˆ†ç±»'),
                    'åˆŠå·': row.get('åˆŠå·'),
                    'æ˜¯å¦å…¥é€‰': row.get('æ˜¯å¦å…¥é€‰', False)
                }
            }

            # åˆ›å»ºchunksï¼ˆä½¿ç”¨é»˜è®¤çš„full_articleç­–ç•¥ï¼‰
            chunks = self.text_processor.create_chunks(doc)

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            for chunk in chunks:
                chunk_dict = {
                    'chunk_id': chunk.chunk_id,
                    'doc_id': chunk.doc_id,
                    'text': chunk.text,
                    'metadata': chunk.metadata,
                    'strategy': chunk.strategy.value,
                    'position': chunk.position,
                    'length': chunk.length
                }
                chunks_data.append(chunk_dict)

        return chunks_data

    def _generate_embeddings(self, chunks_data: List[Dict]) -> List[np.ndarray]:
        """æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡"""
        texts = [chunk['text'] for chunk in chunks_data]
        embeddings = []

        # æ‰¹é‡å¤„ç†
        batch_size = self.embedding_model.batch_size
        for i in tqdm(range(0, len(texts), batch_size), desc="ç”ŸæˆåµŒå…¥"):
            batch_texts = texts[i:i + batch_size]
            batch_embeddings = self.embedding_model.encode_corpus(
                batch_texts,
                show_progress_bar=False
            )
            embeddings.extend(batch_embeddings)

        return embeddings

    def _store_to_vectordb(self, chunks_data: List[Dict], embeddings: List[np.ndarray]):
        """å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“"""
        # å‡†å¤‡æ•°æ®
        ids = [chunk['chunk_id'] for chunk in chunks_data]
        documents = [chunk['text'] for chunk in chunks_data]
        metadatas = [chunk['metadata'] for chunk in chunks_data]

        # æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®
        for i, metadata in enumerate(metadatas):
            metadata['chunk_strategy'] = chunks_data[i]['strategy']
            metadata['chunk_position'] = chunks_data[i]['position']
            metadata['chunk_length'] = chunks_data[i]['length']
            # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯å­—ç¬¦ä¸²ï¼ˆChromaDBè¦æ±‚ï¼‰
            for key, value in metadata.items():
                if value is None:
                    metadata[key] = ""
                elif isinstance(value, (int, float, bool)):
                    metadata[key] = str(value)

        # æ‰¹é‡æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        batch_size = 100
        for i in tqdm(range(0, len(ids), batch_size), desc="å­˜å‚¨åˆ°å‘é‡åº“"):
            batch_ids = ids[i:i + batch_size]
            batch_embeddings = embeddings[i:i + batch_size]
            batch_documents = documents[i:i + batch_size]
            batch_metadatas = metadatas[i:i + batch_size]

            self.vector_store.add(
                ids=batch_ids,
                embeddings=[emb.tolist() for emb in batch_embeddings],
                documents=batch_documents,
                metadatas=batch_metadatas
            )

        logger.info(f"âœ“ æˆåŠŸå­˜å‚¨ {len(ids)} ä¸ªå‘é‡")

    def _save_processed_data(self, df: pd.DataFrame, chunks_data: List[Dict], output_path: Path):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        # ä¿å­˜åŸå§‹å¤„ç†åçš„DataFrame
        df.to_parquet(output_path / "processed_documents.parquet")
        logger.info(f"âœ“ ä¿å­˜æ–‡æ¡£æ•°æ®: processed_documents.parquet")

        # ä¿å­˜chunksæ•°æ®
        chunks_df = pd.DataFrame(chunks_data)
        chunks_df.to_parquet(output_path / "chunks_data.parquet")
        logger.info(f"âœ“ ä¿å­˜chunksæ•°æ®: chunks_data.parquet")

        # ä¿å­˜é…ç½®ä¿¡æ¯
        process_info = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_documents': len(df),
            'total_chunks': len(chunks_data),
            'embedding_model': self.embedding_model.get_model_info(),
            'chunk_strategies': self.text_processor.get_enabled_strategies(),
            'vector_store': {
                'collection_name': self.config['vector_store']['collection_name'],
                'persist_directory': self.config['vector_store']['persist_directory']
            }
        }

        with open(output_path / "process_info.json", 'w', encoding='utf-8') as f:
            json.dump(process_info, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ“ ä¿å­˜å¤„ç†ä¿¡æ¯: process_info.json")

    def _build_bm25_index(self, chunks_data: List[Dict]):
        """æ„å»ºBM25ç´¢å¼•"""
        # å‡†å¤‡æ–‡æ¡£åˆ—è¡¨
        documents = []
        for chunk in chunks_data:
            doc = {
                'id': chunk['chunk_id'],
                'doc_id': chunk['doc_id'],
                'text': chunk['text'],
                'metadata': chunk['metadata']
            }
            documents.append(doc)

        # åˆ›å»ºä¸´æ—¶çš„RAGå¼•æ“æ¥æ„å»ºç´¢å¼•
        from src.rag.engine import RAGEngine
        temp_rag = RAGEngine(
            collection_name=self.config['vector_store']['collection_name'],
            persist_directory=self.config['vector_store']['persist_directory'],
            enable_bm25=True
        )

        # æ„å»ºBM25ç´¢å¼•
        temp_rag.build_bm25_index(documents, force_rebuild=True)
        logger.info(f"âœ“ BM25ç´¢å¼•æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£")

    def _print_statistics(self, df: pd.DataFrame, chunks_data: List[Dict]):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"  åŸå§‹æ–‡æ¡£æ•°: {len(df)}")
        print(f"  ç”Ÿæˆchunksæ•°: {len(chunks_data)}")
        print(f"  å¹³å‡æ¯æ–‡æ¡£chunksæ•°: {len(chunks_data) / len(df):.1f}")

        # Chunkç­–ç•¥ç»Ÿè®¡
        strategy_counts = {}
        for chunk in chunks_data:
            strategy = chunk['strategy']
            strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1

        print("\n  Chunkç­–ç•¥åˆ†å¸ƒ:")
        for strategy, count in strategy_counts.items():
            print(f"    {strategy}: {count} ({count / len(chunks_data) * 100:.1f}%)")

        # é•¿åº¦ç»Ÿè®¡
        lengths = [chunk['length'] for chunk in chunks_data]
        print(f"\n  Chunké•¿åº¦ç»Ÿè®¡:")
        print(f"    å¹³å‡: {np.mean(lengths):.0f} å­—ç¬¦")
        print(f"    æœ€å°: {np.min(lengths)} å­—ç¬¦")
        print(f"    æœ€å¤§: {np.max(lengths)} å­—ç¬¦")
        print(f"    ä¸­ä½æ•°: {np.median(lengths):.0f} å­—ç¬¦")

    def test_retrieval(self, query: str, top_k: int = 5):
        """æµ‹è¯•æ£€ç´¢åŠŸèƒ½"""
        logger.info(f"\nğŸ” æµ‹è¯•æ£€ç´¢: '{query}'")

        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self.embedding_model.encode_queries(query)

        # æ£€ç´¢
        results = self.vector_store.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=top_k
        )

        # æ˜¾ç¤ºç»“æœ
        if results['ids'][0]:
            print(f"\næ‰¾åˆ° {len(results['ids'][0])} ä¸ªç›¸å…³ç»“æœ:")
            for i, (doc_id, distance, document, metadata) in enumerate(zip(
                    results['ids'][0],
                    results['distances'][0],
                    results['documents'][0],
                    results['metadatas'][0]
            )):
                print(f"\n[{i + 1}] ç›¸ä¼¼åº¦: {1 - distance:.3f}")
                print(f"  æ ‡é¢˜: {metadata.get('æ–‡ç« åç§°+å‰¯æ ‡é¢˜', 'N/A')}")
                print(f"  ä½œè€…: {metadata.get('ä½œè€…åç§°', 'N/A')}")
                print(f"  å¹´ä»½: {metadata.get('å¹´ä»½', 'N/A')}")
                print(f"  å†…å®¹é¢„è§ˆ: {document[:200]}...")
        else:
            print("æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")




def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ„å»ºRAGç³»ç»Ÿ')
    parser.add_argument('--input', type=str, default='data/raw/applied_arts.xlsx',
                        help='è¾“å…¥çš„Excelæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default='data/processed',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--config', type=str, default='config.yaml',
                        help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--rebuild', action='store_true',
                        help='æ˜¯å¦é‡å»ºå‘é‡åº“')
    parser.add_argument('--test', action='store_true',
                        help='æ„å»ºåè¿›è¡Œæµ‹è¯•')
    parser.add_argument('--chunk-strategy', type=str, default='full_article',
                        choices=['full_article', 'paragraph_based', 'fixed_size'],
                        help='Chunkåˆ’åˆ†ç­–ç•¥')

    args = parser.parse_args()

    # åˆ›å»ºæ„å»ºå™¨
    builder = RAGBuilder(args.config)
    builder.rebuild = args.rebuild

    # è®¾ç½®chunkç­–ç•¥
    if args.chunk_strategy != 'full_article':
        builder.text_processor.set_strategy(args.chunk_strategy)
        logger.info(f"åˆ‡æ¢chunkç­–ç•¥ä¸º: {args.chunk_strategy}")

    # å¤„ç†æ•°æ®
    builder.process_data(args.input, args.output)

    # æµ‹è¯•æ£€ç´¢
    if args.test:
        print("\n" + "=" * 70)
        print("æµ‹è¯•æ£€ç´¢åŠŸèƒ½")
        print("=" * 70)

        test_queries = [
            "åŒ…è±ªæ–¯çš„è®¾è®¡ç†å¿µ",
            "ç°ä»£ä¸»ä¹‰å»ºç­‘",
            "å·¥ä¸šè®¾è®¡çš„å‘å±•",
            "ä¸­å›½ä¼ ç»Ÿå·¥è‰º"
        ]

        for query in test_queries:
            builder.test_retrieval(query, top_k=3)
            print("\n" + "-" * 50)


if __name__ == "__main__":
    main()